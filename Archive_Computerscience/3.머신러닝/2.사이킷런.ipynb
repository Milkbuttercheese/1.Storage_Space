{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## CH2 사이킷런\n\n## 1. 붓꽃 품종 예측하기\n- 분류 classification은 대표적인 지도학습 supervised learning 방법이다\n- 지도학습 방법은  정답 데이터를 제공하면서 모델을 학습시키는 방법이다\n\n- 이번 예제에서 사용되는  `sklearn` 라이브러리속 모듈\n\t- `sklearn.datasets`: 사이킷런에서 제공하는 데이터셋이 저장되어 있는 모듈\n\t- `sklearn.model_selection`: 데이터를 학습 데이터와 검증 데이터, 예측 데이터로 분리하거나 최적의 하이퍼 파라미터로 평가하기 위해 사용되는 함수들이 저장되어 있는 모듈\n\t\t- `sklearn.model_selection import train_test_split` : 데이터를 학습용과 테스트용으로 분리시켜주는 함수\n\t\t- `shuffle=True`: 데이터를 분리하기전에 데이터를 미리 섞을 지 정하는 옵션\n\t-   `sklearn.tree`: 트리 기반 ML 알고리즘이 저장되어 있는 모듈\n\t\t- `sklearn.tree.DecisionTreeClassifier`: 의사결정 트리 알고리즘 메소드\n\n### 1. 데이터 전처리\n- `sklearn`에서 `iris` 데이터 불러오기\n\t- `iris.data` : `iris`의 독립변수 데이터\n\t- `iris.target`: `iris`의 종속변수 데이터(정수로 표기됨)\n\t- `iris.target_names`: 종속변수 데이터(문자열로 표기됨)\n\t- `iris.feature_names`: 피쳐명(수학-계수/데이터프레임-컬럼명)\n- 가져온 데이터를 데이터프레임에 넣어 정리하기\n```python\niris_df= pd.DataFrame(data=iris_data,columns=iris.feature_names)  \niris_df['label']=iris.target\n```\n- 데이터를 학습용 데이터와 테스트용 데이터로 분리하기\n```python\nX_train, X_test, y_train, y_test = train_test_split(iris_data, iris_label,test_size=size_val,random_state=11)\n```\n- \n\t- `train_test_split`의 `argument`로 독립변수,종속변수,나누어질 테스트 데이터셋의 비율을 필요로 한다\n\n### 2. 머신러닝 알고리즘을 활용하여 학습 및 예측하기\n- DecisionTreeClassifier 알고리즘 활용하기\n\t-  모델 객체 생성$\\rightarrow$ 학습 수행 $\\rightarrow$ 예측 수행 $\\rightarrow$ 정확도 평가\n```python \n#모델 생성하기  \nDTC= DecisionTreeClassifier(random_state=11)  \n#모델 학습시키기  \nDTC.fit(X_train,y_train)  \n#학습이 완료된 모델 객체와 테스트 데이터셋을 활용하여 예측하기  \npred= DTC.predict(X_test)  \n#정확도 평가하기  \nprint(f'예측 정확도: {accuracy_score(y_test,pred):.3f}')\n\n________________________________________\nresult)\n예측 정확도: 0.933\n```\n\n## 2. 사이킷런의 기반 프레임워크 \n- Estimator\n\t-  Classifier : 분류 알고리즘을 구현한 클래스\n\t-  Regressor: 회귀 알고리즘을 구현한 클래스 \n\t- Estimator는 학습을 위하여 fit(), 예측을 위하여 predict() 메소드를 사용하도록 통일하였다\n- 관련 용어들\n\t- Hyperparameter : 학습 이전에 조정되며, 모델 학습에 영향을 주는 변수\n\t- parameter: 학습에 의해 결정되어 예측에 활용되는 변수\n\n### 사이킷런의 주요 모듈\n- 예제 데이터 \n\t- `sklearn.datasets`: 사이킷런에 내장되어 있는 데이터셋을 제공한다. 데이터셋은 딕셔너리 형태로 주어진다\n- 피처 처리\n\t- `sklearn.preprocessing`: 데이터 전처리에 필요한 다양한 가공 기능을 제공한다\n\t\t- 문자열을 숫자형 코드로 인코딩\n\t\t- 정규화\n\t\t- 스케일링\n\t- `sklearn.feature_selection` : 영향력이 큰 피쳐가 무엇인지 특정 기준으로 판별하여 취사선택할 수 있게 해주는 기능을 제공한다\n- 차원 축소\n\t- `sklearn.decomposition` : 차원 축소와 관련된 알고리즘을 제공한다\n- 데이터 분리/검증/파라미터 튜닝\n\t- `sklearn.model_selection` : 교차 검증을 위한 학습/테스트용 데이터 분리, 최적 파라미터 추출을 위한 API등을 제공한다\n- 평가\n\t- `sklearn.metrics` : 학습된 모델의 예측 성능 측정 방법을 제공한다\n- 머신러닝 알고리즘\n\t- `sklearn.ensemble` : 앙상블 알고리즘 제공\n\t- `sklearn.linear_model` : 회귀 관련 알고리즘을 제공\n\t\t- 선형 회귀\n\t\t- 릿지 Ridge 회귀\n\t\t- 라쏘 Lasso 회귀\n\t\t- 로지스틱 회귀 \n\t- `sklearn.naive_bayes` : 나이브 베이즈 알고리즘 제공\n\t- `sklearn.neighbors` : 최근접 이웃 알고리즘 제공\n\t- `sklearn.svm` : 서포트 벡터 머신 알고리즘 제공\n\t- `sklearn.tree` : 의사 결정 트리 알고리즘 제공\n\t- `sklearn.cluster` : 비지도 클러스터링 알고리즘 제공\n\n\n## 3.`model_selection` 모듈 \n- 과적합overfitting\n\t- 모델이 학습 데이터에 과도하게 최적화되어, 학습 데이터 외의 다른 데이터로 예측할 때엔 예측 성능이 과도하게 떨어지는 현상. \n\t- 만약 학습 데이터와 테스트 데이터를 고정한 채로 평가 및 수정하다보면 테스트 데이터에만 최적의 성능을 발휘할 수 있도록 편향되게 모델을 유도하는 경향이 생긴다. 이런 경우 제 3의 데이터에 대한 과적합 문제가 발생하는 것이다\n\t- 이러한 문제점을 해결하기 위해 교차검증을 통해 다양한 학습과 평가를 수행한다\n- 교차 검증\n\t- 데이터를 훈련/ 테스트 데이터로 나누고 훈련 데이터의 일부를 검증 데이터로 할당한다. 테스트 데이터는 모든 학습/검증 과정이 완료된 이후 최종적으로 성능 평가를 위해 사용하는 데이터 셋이 된다\n- K-폴드 교차 검증\n\t- 우선 훈련 데이터를 5등분 한다. 4조각 중 하나를 훈련 데이터로, 한조각을 검증 데이터로 하는데 한번의 평가를 마칠때마다 검증 데이터를 다음 조각으로 바꾼다\n\t- 예시)\n```python\n#전체 데이터는 150 => 학습은 150*4/5=120, 검증은 30n_iter=0  \n# KFold 객체의 split()을 호출하면 폴드용 학습용, 검증용 테스트의 로우 인덱스를 array로 반환한다  \nfor train_index, test_index in kfold.split(features):  \n    #kfold.split()으로 반환된 인덱스를 이용해 학습용, 검증용 테스트 데이터 추출  \n    X_train, X_test = features[train_index], features[test_index]  \n    y_train, y_test= label[train_index], label[test_index]  \n    #학습 및 예측  \n    DTC.fit(X_train,y_train)  \n    pred=DTC.predict(X_test)  \n    n_iter= n_iter+1  \n    #반복시마다 정확도를 측정한다  \n    accuracy= np.round(accuracy_score(y_test,pred),4)  \n    train_size= X_train.shape[0]  \n    test_size= X_test.shape[0]  \n    print(f'{n_iter} 교차 검증 정확도: {accuracy} 학습 데이터 크기: {train_size} 검증 데이터 크기{test_size}')  \n    print(f'{n_iter} 검증 세트 인덱스{test_index}')  \n    cv_accuracy.append(accuracy)  \n  \n#반복할 때마다 계산된 정확도를 평균한다  \nprint(f'평균 검증 정확도{np.mean(cv_accuracy)}')\n\n__________________________________________________________\nresult)\n1 교차 검증 정확도: 1.0 학습 데이터 크기: 120 검증 데이터 크기30\n1 검증 세트 인덱스[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n 24 25 26 27 28 29]\n2 교차 검증 정확도: 0.9667 학습 데이터 크기: 120 검증 데이터 크기30\n2 검증 세트 인덱스[30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53\n 54 55 56 57 58 59]\n3 교차 검증 정확도: 0.8667 학습 데이터 크기: 120 검증 데이터 크기30\n3 검증 세트 인덱스[60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83\n 84 85 86 87 88 89]\n 4 교차 검증 정확도: 0.9333 학습 데이터 크기: 120 검증 데이터 크기30\n4 검증 세트 인덱스[ 90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n 108 109 110 111 112 113 114 115 116 117 118 119]\n5 교차 검증 정확도: 0.7333 학습 데이터 크기: 120 검증 데이터 크기30\n5 검증 세트 인덱스[120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137\n 138 139 140 141 142 143 144 145 146 147 148 149]\n평균 검증 정확도0.9\n\n``` \n- Stratified K 폴드 검증 \n\t- 일반적으로 각 카테고리별 데이터의 수가 균질하지 않기 때문에 범용적으로 사용되는 \n```python\nfrom sklearn.model_selection import StratifiedKFold    \nskf= StratifiedKFold(n_splits=3)  \nn_iter=0  \n  \n#레이블 분포도에 따라 학습/검증 데이터를 나누기 때문에 split argument에 레이블 데이터도 필요하다  \nfor train_index, test_index in skf.split(iris_df, iris_df['label']):  \n    n_iter = n_iter +1  \n    label_train = iris_df['label'].iloc[train_index]  \n    label_test = iris_df['label'].iloc[test_index]  \n    print(f'교차 검증: {n_iter}')  \n    print(f'학습 레이블 데이터 분포\\n', label_train.value_counts())  \n    print(f'검증 레이블 데이터 분포\\n', label_test.value_counts())\n\n_____________________________________________________\nresult)\n교차 검증: 1\n학습 레이블 데이터 분포\n 2    34\n0    33\n1    33\nName: label, dtype: int64\n검증 레이블 데이터 분포\n 0    17\n1    17\n2    16\nName: label, dtype: int64\n교차 검증: 2\n학습 레이블 데이터 분포\n 1    34\n0    33\n2    33\nName: label, dtype: int64\n검증 레이블 데이터 분포\n 0    17\n2    17\n1    16\nName: label, dtype: int64\n교차 검증: 3\n학습 레이블 데이터 분포\n 0    34\n1    33\n2    33\nName: label, dtype: int64\n검증 레이블 데이터 분포\n 1    17\n2    17\n0    16\nName: label, dtype: int64\n```\n- 교차 검증 활용 예시\n``` python\nfrom sklearn.model_selection import StratifiedKFold  \n  \nskf= StratifiedKFold(n_splits=3)  \nn_iter=0  \ncv_accuracy=[]  \n  \n#레이블 분포도에 따라 학습/검증 데이터를 나누기 때문에 split argument에 레이블 데이터도 필요하다  \nfor train_index, test_index in skf.split(features,label):  \n    #split()으로 반환된 인덱스를 이용해 학습용, 검증용 테스트 데이터를 추출하기  \n    X_train, X_test= features[train_index], features[test_index]  \n    y_train, y_test = label[train_index], label[test_index]  \n    #학습 및 예측  \n    DTC.fit(X_train, y_train)  \n    pred= DTC.predict(X_test)  \n  \n    #반복시마다 정확도 측정  \n    n_iter=n_iter +1  \n    accuracy= np.round(accuracy_score(y_test,pred),4)  \n    train_size= X_train.shape[0]  \n    test_size= X_test.shape[0]  \n    print(f'\\n {n_iter} 교차 검증 정확도:{accuracy}, 학습 데이터 크기: {train_size}, 검증 데이터 크기:{test_size}')  \n    print(f'{n_iter} 검증 세트 인덱스 {test_index}')  \n    cv_accuracy.append(accuracy)  \n  \n#교차 검증별 정확도 및 평균 정확도 계산  \nprint('\\n 교차 검증별 정확도:' ,np.round(cv_accuracy,4))  \nprint('평균 검증 정확도:',np.round(np.mean(cv_accuracy),4))\n\n___________________________________________________________\nresult)\n1 교차 검증 정확도:0.98, 학습 데이터 크기: 100, 검증 데이터 크기:50\n1 검증 세트 인덱스 [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  50\n  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66 100 101\n 102 103 104 105 106 107 108 109 110 111 112 113 114 115]\n\n 2 교차 검증 정확도:0.94, 학습 데이터 크기: 100, 검증 데이터 크기:50\n2 검증 세트 인덱스 [ 17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  67\n  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82 116 117 118\n 119 120 121 122 123 124 125 126 127 128 129 130 131 132]\n\n3 교차 검증 정확도:0.98, 학습 데이터 크기: 100, 검증 데이터 크기:50\n3 검증 세트 인덱스 [ 34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  83  84\n  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 133 134 135\n 136 137 138 139 140 141 142 143 144 145 146 147 148 149]\n\n 교차 검증별 정확도: [0.98 0.94 0.98]\n평균 검증 정확도: 0.9667\n```\n\n## 4. 교차검증 모듈 `cross_val_score`\n- 일반적으로 교차검증을 시행하기 위해선 \n\t1. 폴드 세트를 설정한다\n\t2. for 루프에서 반복으로 훈련 및 테스트 데이터의 인덱스를 추출한다\n\t3. 반복적으로 모델 학습과 예측을 수행하고 예측 성능을 반환한다\n- `cross_val_score` 는 이런 데이터 분리, 모델 학습, 정확도 평가의 과정을 한꺼번에 시행해준다\n```python\niris_data= load_iris()  \nDTC= DecisionTreeClassifier(random_state=156)  \n  \ndata= iris_data.data  \nlabel= iris_data.target  \n  \n#성능 지표는 정확도(accuracy), 교차 검증 세트는 3개로 정한다  \nscores=cross_val_score(DTC,data,label,scoring='accuracy',cv=3)  \nprint('교차 검증별 정확도:',np.round(scores,4))  \nprint('평균 검증 정확도',np.round(np.mean(scores),4))\n___________________________\nresult)\n교차 검증별 정확도: [0.98 0.94 0.98]\n평균 검증 정확도 0.9667\n```\n\n## 5.`GridSearchCV`- 교차검증과 최적 하이퍼파라미터 튜닝을 한번에 하기 \n- 사이킷런은 `GridSearchCV `API를 통하여 머신러닝 알고리즘에 사용되는 하이퍼 파라미터를 순차적으로 입력하면서 편리하게 최적의 파라미터를 탐색할 수 있는 방법을 제공한다. \n``` python\ngrid_parameters= {'hyper_par1':[x_1,x_2,...,x_n],\n\t\t\t\t  'hyper_par2':[y_1,y_2,...,y_m]}\n```\n- 이라 설정할 경우 `hypter_par1`과 `hyper_par2`의 이중 for문 형식으로 데이터가 차례대로 대입되게 된다\n-  그리드서치 예시\n- 학습시킨 `grid_dtree`의 `.cv_results_`를 데이터프레임으로 만들었을 때\n\t- `params` 열은 수행에 적용된 개별 하이퍼 파라미터이다\n\t- `rank_test_score` 는 하이퍼 파라미터별로 성능이 좋은 score의 순위이다\n\t- `mean_test_score`는 개별 하이퍼 파라미터별로 CV 폴딩 테스트셋에 대한 평가 평균값이다\n```python\nfrom sklearn.datasets import load_iris  \nfrom sklearn.tree import DecisionTreeClassifier  \nfrom sklearn.model_selection import GridSearchCV  \n  \n#데이터를 로딩하고 학습 데이터와 테스트 데이터를 분리시킨다  \niris_data= load_iris()  \nX_train, X_test, y_train, y_test = train_test_split(iris_data.data,iris_data.target,test_size=0.2,random_state=121)  \n  \ndtree=DecisionTreeClassifier()  \n  \n#하이퍼 파라미터를 딕셔너리 형태로 만든다  \nparameters= {'max_depth':[1,2,3],'min_samples_split':[2,3]}  \n  \n#parameter_grid의 하이퍼 파라미터를 3개의 train<test set fold로 나누어 테스트를 수행한다  \ngrid_dtree= GridSearchCV(dtree,param_grid=parameters, cv=3 ,refit=True)  \n  \n#붓꽃 학습 데이터로 param_grid의 하이퍼 파라미터를 순차적으로 학습 및 평가  \ngrid_dtree.fit(X_train,y_train)  \n  \n#GridsearchCV 결과를 추출하여 DataFrame으로 변환시킨다  \nscores_df= pd.DataFrame(grid_dtree.cv_results_)  \nscores_df[['params','mean_test_score','rank_test_score','split0_test_score','split1_test_score','split2_test_score']]\n\n______________________________________________________\nresult)\nparams/mean_test_score/rank_test_score/split0_test_score/split1_test_score/split2_test_score\n0,\"{'max_depth': 1, 'min_samples_split': 2}\",0.700000,5,0.700,0.7,0.70\n1,\"{'max_depth': 1, 'min_samples_split': 3}\",0.700000,5,0.700,0.7,0.70\n2,\"{'max_depth': 2, 'min_samples_split': 2}\",0.958333,3,0.925,1.0,0.95\n3,\"{'max_depth': 2, 'min_samples_split': 3}\",0.958333,3,0.925,1.0,0.95\n4,\"{'max_depth': 3, 'min_samples_split': 2}\",0.975000,1,0.975,1.0,0.95\n5,\"{'max_depth': 3, 'min_samples_split': 3}\",0.975000,1,0.975,1.0,0.95\n```\n- 최적 파라미터와 최고 정확도 출력하기\n```python\nprint('GridSearchCV 최적 파라미터는 ',grid_dtree.best_params_)  \nprint(f'GridSearchCV 최고 정확도는 {grid_dtree.best_score_:.3f}')\n\n________________________________________________________\nresult)\nGridSearchCV 최적 파라미터는  {'max_depth': 3, 'min_samples_split': 2}\nGridSearchCV 최고 정확도는 0.975\n```\n- 최적 학습된 모델을 반환받아 테스트 셋으로 예측 및 성능 평가해보기\n```python\n\n#GridSearchCV의 reft으로 이미 학습된 estimator 반환시키기  \nestimator = grid_dtree.best_estimator_  \n  \n#GridSearchCV의 best_estimator_는 이미 최적 학습이 되었으므로 별도의 학습이 필요없다  \npred= estimator.predict(X_test)  \nprint(f'테스트 데이터 셋의 정확도는 {accuracy_score(y_test,pred):.4f} 입니다')\n\n________________________________________________________________________\nresult)\n테스트 데이터 셋의 정확도는 0.9667 입니다\n```\n\n## 6.  데이터 전처리\n- 결손값 NaN\n\t- 결손값 NaN의 존재는 허용되지 않는다\n\t- 결손값의 존재가 얼마 되지 않는다면 해당 열의 평균 값으로 대체하는 등의 방식이 존재한다\n\t- 그러나 결손값이 대부분이라면 차라리 해당 열을 드롭하는 것이 나을 수도 있다\n- 비수치형-문자열 독립변수\n\t- 사이킷런 머신러닝 알고리즘은 문자열 값을 입력값으로 허용하지 않는다\n\t- 그러므로 모든 문자열은 인코딩 되어 숫자형으로 변환되어야 한다\n\t\t- 레이블 인코딩\n\t\t\t- 각각의 문자열 카테고리를 정수 카테고리 에 대응시켜 변환시킨다.\n\t\t\t- `preprocessing.LabelEncoder`를 사용한다\n\t\t\t\t- `encoder=LabelEncoder()` 선언\n\t\t\t\t- `encoder.fit(encoding_lst)`  대입\n\t\t\t\t- `labels=encoder.transform(encoding_lst)` 을 통해 변환된 정수 카테고리값을 `label`에 저장한다\n\t\t\t\t- `encoded_class=encoder.classes_` 를 통해 인코딩 되기전 문자열 카테고리명을 정수 카테고리 순서대로로 불러온다. \n\t\t\t\t- `encoder.inverse_transform`을 이용하면 정수값에 대응되는 문자열 카테고리를 불러온다\n\t\t\t-  **명심할 사항: 정수형 카테고리의 정숫값의 크기가 알고리즘의 영향을 주는 일이 발생해선 안된다. 이러한 특성으로 레이블 인코딩은 회귀 알고리즘에 사용되지 않는다. 그러나 트리 계열 알고리즘은 이런 문제가 발생하지 않으므로 사용할 수 있다 \n\t\t- 원 핫 인코딩 One-Hot Encoding\n\t\t\t- 피처의 갯수만큼의 길이를 가진 튜플을 만들고, 각 카테고리를 원소 위치와 대응시킨다. 해당 카테고리의 특성을 만족시키면 1, 아니면 0으로 표기한다\n\t\t\t- `preprocessing.OneHotEncoder`를 사용한다\n\t\t\t\t- `encoding_lst=np.array(encoding_lst).reshape(-1,1)` 를 통해 2차원 배열을 만들어 줘야 한다\n\t\t\t\t- `oh_encoder=OneHotEncoder()` 선언\n\t\t\t\t- `oh_encdoer.fit(encoding_lst)` 대입\n\t\t\t\t- `labels=oh_encoder,transform(encoding_lst)` 를 통해 변환된 희소행렬 카테고리 값을 `label`에 저장한다 이후 `labels.toarray()`를 통해 밀집행렬로 출력한다\n\t\t\t\t\n- 피쳐 스케일링 feature scaling\n\t- 서로 다른 변수 값 범위를 일정한 수준으로 맞춰주는 작업\n\t- 대표적으로 표준화 standardzation과 정규화 normalization가  존재한다\n\t- 표준화 standardzation\n\t\t- 각 열의 데이터값을 평균이 0, 분산이 1인 가우시안 정규 분포를 가지는 값으로 변환시키는 것\n\t\t- $z_{i}=\\cfrac{x_i-\\bar{x}}{\\sigma_x}$\n\t- 정규화 Normalization\n\t\t- 각 열의 데이터값을 최소 0~ 최대 1의 값으로 변환하는 것\n\t\t\t- $z_i=\\cfrac{x_i}{max(x)-min(x)}$\n\t- 벡터 정규화 vector nomalization\n\t\t- 사이킷런에서 제공하는 Normalizaer모듈은 일반적 정의의 정규화와는 정의가 다르다. 이는 선형대수의 정규화와 대응되는 것으로 개별 벡터를 모든 피쳐 벡터의 크기로 나누어준다. *이를 벡터 정규화*로 따로 구분하여 부르기로 약속한다\n\t\t- 원소가 서로 다른 단위를 가지고 있으면 사용하지 않는다. 일반적으론 딥러닝의 학습벡터에 사용된다\n\t\t-  $\\tilde{x_i}=\\cfrac{x_i}{\\sqrt{x_i^2+y_i^2+z_i^2}}$\n- 표준화 예시)\n```python\n#StandardScaler  \nfrom sklearn.datasets import load_iris  \nimport pandas as pd  \n#붓꽃 데이터를 로딩하고 Dataframe으로 변환한다  \niris= load_iris()  \niris_data= iris.data  \niris_df = pd.DataFrame(data=iris_data,columns=iris.feature_names)  \n  \nprint(f'feature들의 평균값은 \\n{iris_df.mean()}')  \nprint(f'\\nfeature들의 분산값은 \\n{iris_df.var()}')\n\n#이제 StandardScaler를 활용하여 정규화하기  \nfrom sklearn.preprocessing import StandardScaler  \n  \n#StandardScaler 객체 생성하기  \nscaler= StandardScaler()  \n#StandardScaler로 데이터 세트 변환. fit() 과 trnasformation() 호출  \nscaler.fit(iris_df)  \niris_scaled=scaler.transform(iris_df)  \n  \n#transform()시 스케일 변환된 데이터 셋이 넘파이 배열로 변환되어 이를 데이터프레임으로 변환한다  \niris_df_scaled= pd.DataFrame(data=iris_scaled,columns=iris.feature_names)  \nprint(f'features의 평군값은 \\n{iris_df_scaled.mean()}\\n이다')  \nprint(f'feature들의 분산 값은 \\n{iris_df_scaled.var()}\\n이다')\n____________________________________________________\nresult)\nfeatures의 평군값은 \nsepal length (cm)   -1.690315e-15\nsepal width (cm)    -1.842970e-15\npetal length (cm)   -1.698641e-15\npetal width (cm)    -1.409243e-15\ndtype: float64\n이다\nfeature들의 분산 값은 \nsepal length (cm)    1.006711\nsepal width (cm)     1.006711\npetal length (cm)    1.006711\npetal width (cm)     1.006711\ndtype: float64\n이다\n\n```\n- 정규화 예시\n``` python\nfrom sklearn.preprocessing import MinMaxScaler  \n  \n#MinMaxScaler객체 생성하기  \nscaler= MinMaxScaler()  \n#MinMaxSclaer로 데이터 세트 변환. fit()과 transformation() 호출하기  \nscaler.fit(iris_df)  \niris_scaled= scaler.transform(iris_df)  \n  \n#transform()시 스케일 변환된 데이터 셋이 넘파이 배열로 반환되어 이를 데이터프레임으로 변환시킨다  \niris_df_scaled= pd.DataFrame(data=iris_scaled,columns=iris.feature_names)  \nprint(f'feature들의 최솟값은 \\n{iris_df_scaled.min()}\\n입니다')  \nprint(f'feature들의 최댓값은 \\n{iris_df_scaled.max()}\\n입니다')\n\nresult)\n_______________________________________________\nfeature들의 최솟값은 \nsepal length (cm)    0.0\nsepal width (cm)     0.0\npetal length (cm)    0.0\npetal width (cm)     0.0\ndtype: float64\n입니다\nfeature들의 최댓값은 \nsepal length (cm)    1.0\nsepal width (cm)     1.0\npetal length (cm)    1.0\npetal width (cm)     1.0\ndtype: float64\n입니다\n```\n- *피쳐 스케일링 시 유의점*\n\t- fit()은 데이터 변환을 위한 기준정보를 설정하고, transformation()은 이렇게 설정된 정보를 이용해 데이터를 변환한다.\n\t- 만약 학습 데이터셋과 테스트 데이터셋에 피쳐 스케일링을 적용시킨다면, 테스트 데이터셋에 transform()을 시행시킬 때 학습 데이터에 적용시킨 fit() 을 사용해야 한다.\n\t- 만약 그렇게 안한다면 같은 데이터값을 가지고 있는데도, 다른 fit()함수를 가지고 있어 변환값이 다르고, 그 결과 예측값이 다르게 생기는 문제가 발생할 것이다\n\n\n## 7. 타이타닉 예제- 데이터 일괄 전처리 모듈 만들기\n```python\n#데이터 전처리: Null값 처리/ 불필요 피처 제거/ 문자열-카테고리 인코딩을 일괄적으로 수행할 수 있겠끔 함수로 만들기  \n  \n# 결측치 처리하기  \ndef fillna(df):  \n    # 이 책에선 age는 열의 평균으로, 나머지 변수들은 'N'값으로 체우기로 결정한다  \n    df['Age'].fillna(df['Age'].mean(),inplace=True)  \n    df['Cabin'].fillna('N',inplace=True)  \n    df['Embarked'].fillna('N',inplace=True)  \n    return df  \n  \n#불필요해보이는 피처 제거하기  \ndef drop_features(df):  \n    df.drop(['PassengerId','Name','Ticket'],axis=1,inplace=True)  \n    return df  \n  \n#문자열- 카테고리 피처 Sex,Cabin,Embarked를 인코딩하기  \ndef encoding_features(df):  \n    #한번만 인코딩해야됨. 두번하면 안됨  \n    categories_backup=[]  \n    features=['Cabin','Sex','Embarked']  \n    for feature in features:  \n        LE= LabelEncoder()  \n        LE= LE.fit(df[feature])  \n        #카테고리 백업해두기  \n        categories_backup.append(list(LE.classes_))  \n        #인코딩하기  \n        df[feature]= LE.transform(df[feature])  \n    return df  \n  \n#앞에서 정의한 전처리 함수들을 호출하여 한번에 전처리를 완료하기  \ndef preprocessing_feature(df):  \n    df= fillna(df)  \n    df= drop_features(df)  \n    df= encoding_features(df)  \n    return df  \n  \n  \ndf_titanic= pd.read_csv('./train.csv')  \ny_titanic_df= df_titanic['Survived']  \nX_titanic_df= df_titanic.drop('Survived',axis=1)  \nX_titanic_df= preprocessing_feature(X_titanic_df)\n```\n",
   "metadata": {
    "cell_id": "914acd6fbc0c4627abf7aacbf9d37a32",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 13333.5
   }
  },
  {
   "cell_type": "markdown",
   "source": "",
   "metadata": {
    "cell_id": "a79afe7d269244bf87f7dc083dcc8eef",
    "tags": [],
    "owner_user_id": "6620e961-4734-4451-b6f0-024902f1a0c6",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 46
   }
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=5a548db0-9bec-4ed6-b090-9c062f32efdb' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {},
  "deepnote_notebook_id": "fa2fdfc9-e791-4241-ac01-f4b2ce2f206b",
  "deepnote_execution_queue": []
 }
}
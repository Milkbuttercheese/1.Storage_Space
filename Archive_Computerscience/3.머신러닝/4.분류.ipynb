{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## CH4  분류\n- 지도학습의 대표적 유형중 하나로, 데이터를 통해 학습된 모델이 새로운 데이터 값이 주어졌을 때 그것의 레이블 값을 예측하는 것, 그 기술 및 이론\n- 분류의 머신러닝 알고리즘 종류   \n\t- 나이브 베이즈 Naive Bayes: 베이즈 통계와 생성모델에 기반한 알고리즘 \n\t- 로지스틱 회귀 Logisitc Regression: 독립변수와 종속변수의 선형 관계성의 기반한 회귀 알고리즘\n\t- 결정 트리 Decision Tree: 데이터 균일도에 따른 규칙 기반의 알고리즘\n\t- 서포트 벡터 머신 Support Vector Machine:개별 클래스 간의 최대 분류 마진을 효과적으로 찾아주는 알고리즘\n\t- 최소 근접 알고리즘 Nearest Neighbor: 근접 거리를 기준으로 하는 최소 근접 알고리즘\n\t- 신경망 Neural Network \n\t- 앙상블 Ensemble: 서로 다른 머신러닝 알고리즘을 결합하여 경합시킨 알고리즘 형태\n\t\t- 이 책(파이썬 머신러닝 완벽가이드: 권철민 저 2판)에서는 다양한 알고리즘 중 *앙상블 방법*을 집중적으로 다룬다.비정형데이터에선 딥러닝이 머신러닝계를 선도하지만 이를 제외한 정형 데이터의 예측 분석에선 앙상블이 매우 높은 예측 성능을 가지고 있기 때문이다.\n\t- 앙상블의 방식\n\t\t- 배깅 Bagging: 대표적으로 랜덤 포레스트 Random Forest 방식이 있다. 뛰어난 예측 성능, 상대적으로 빠른 수행 시간, 유연성 등으로 많은 분석가등으로 애용되고 있다\n\t\t- 부스팅 Boosting: 대표적으로 그래디언트 부스팅 Gradient Boosting등이 있으나 이 알고리즘은 수행 시간이 너무 오래걸리는 단점을 가지고 있었다. 그 후 XGBoost, LightGBM등으로 기존 그래디언트부스팅의 예측 성능을 발전시키면서도 수행 시간을 단축시킨 알고리즘이 개발되고 있다\n\n## 2. 결정 트리\n- 데이터에 있는 규칙을 학습하여, 트리 기반의 분류규칙을 생성하는 알고리즘이다\n- 트리 관련 용어\n    - 루트$root$\n        - 트리의 가장 위쪽에 있는 노드\n    - 리프 $leaf$ / 단말 노드 $terminal\\,\\,node$ / 외부 노드 $external\\,\\,node$\n        - 트리의 가장 아래쪽에 있는 노드\n\t\t- *결정 트리 알고리즘에선 결정된 클래스 값*을 의미한다\n    - 비단말 노드 $non-terminal\\,\\,node$/ 내부 노드$internal\\,\\,node$ \n        - 리프를 제외한 모든 노드(루트를 포함한다)\n\t\t- *결정 트리 알고리즘에선 규칙 노드(Decision Node)* 라고 칭한다\n    - 노드의 가족관계\n        - 어떤 노드 $y$ 가 가지로 연결된 아래쪽 노드 $x_{1},x_{2},...,x_{n}$을 가지고 있다면\n            - $y$를 $x_{1},x_{2},...x_n$의 부모 $parent$라고 부른다\n            - $x_1,x_2,...,x_n$을 $y$ 의 자식 $child$이라 부른다\n            - $x_1,x_2,...,x_n$는 서로 형제 $sibling$이라 부른다\n            - $y$는 $n$개의 자식을 가지고 있고, 이를 다시 말해 차수 $degree=n$이라고 표현한다\n            - $y$에서 위쪽 방향으로만 가지를 타고 이동하였을 때 접하는 노드를 $y$의 조상$ancestor$라고 부른다\n            - $y$에서 아래쪽 방향으로만 가지를 타고 이동하였을 때 접하는 노드를 $y$의 자손$descendant$라 부른다\n            - $y$와 $y$의 자손으로 구성된 트리를 서브트리$subtree$라 부른다\n    - 레벨 $level$\n        - 어떤 노드 $y$가 루트를 향하여 위쪽 방향으로만 가지를 타고 이동하였을 때 몇번만에 도착하였는지를 측정한 값\n    - 높이 $height$\n        - 리프에서 루트까지의 레벨\n    - 빈 트리 $null\\,\\,tree$\n        - 노드와 가지가 전혀 없는 트리를 빈 트리라고 부른다\n    - 순서 트리$ordered\\,\\,tree$와 무순서 트리$unordered\\,\\,tree$\n        - 형제 노드의 순서관계를 구별하면 순서트리, 순서를 구별하지 않으면 무순서 트리라고 부른다\n\n## 평가 기준: 엔트로피와 지니 계수\n\n### 엔트로피\n- 확률분포가 가지는 정보량을 수치로 표현한 것이다. \n- 확률분포에서 특정한 값이 나올 확률이 높아지고, 다른 값이 나올 확률이 낮을수록 엔트로피의 값이 작고 반대로 대부분이 비슷한 경우 엔트로피는 높아지는 경향성을 가진다\n- 엔트로피의 정의\n\t- 이산확률변수인 경우 \n\t\t- $H[Y]= -\\sum_{k=1}^{K}p(y_k)log_2p(y_k)$ \n\t\t\t- 이때 $K$ 는 $X$ 가 가질 수 있는 클래스의 총 수, $p(y_k)$ 는 확률질량함수이다\n\t- 연속확률변수 인경우 \n\t\t- $H[Y]=-\\int_{-\\infty}^{\\infty}p(y)log_2p(y)dy$\n\t\t- 이떄 $P(y)$는 확률밀도함수이다\n- 엔트로피의 성질\n\t- 엔트로피는 확률분포함수를 입력으로 받아 숫자로 출력하는 범함수 $functional$ 이다.\n\t- 엔트로피 계산에서 $p(y)=0$ 인경우 로그값이 정의되지 않으므로 다음과 같은 극한값을 로피탈의 정리를 통하여 구한다. $lim_{p\\rightarrow 0} \\,\\, plog_2 p=0$\n\n### 지니 계수\n- 불순도를 측정하는 지표로서 데이터의 통계적 분산정도를 수치로 표현한 것이다\n- 지니계수의 정의\n\t- $G(S)= 1- \\sum_{i=1}^{C}p_i^2$\n\t- $S$ 는 이미 발생한 사건의 모음, $C$는 사건의 갯수를 의미한다\n\t- [레퍼런스](https://leedakyeong.tistory.com/entry/의사결정나무Decision-Tree-CART-알고리즘-지니계수Gini-Index란)\n\n### 결정트리의 선택 : 결정 트리 파라미터를 결정\n- `min_samples_split`\n\t- 노트를 분할하기 위한 최소한의 샘플 데이터 수를 정하는 파라미터로 과적합을 제어하는데에 사용한다.\n\t- 디폴트 값은 2이다\n- `min_samples_leaf`\n\t- 말단 노드가 되기 위한 최소 샘플 데이터 수를 정하는 파라미터로 과적합을 제어하는데 사용한다 .\n\t- 그러나 비대칭적 데이터의 경우 특정 클레스의 데이터가 극도로 적을 수 있으니 이 경우는 유의해야 한다\n- `max_features`\n\t- 최적의 분할을 위해 고려할 최대 피처 갯수를 정한다. \n\t- 디폴트는 None으로 데이터의 모든 피처를 활용한다\n\t- int 로 지정하면 대상 피처의 갯수를, float으로 지정하면 전체 피처중 대상 피처의 퍼센트를 활용한다\n- `max_depth`\n\t- 트리의 최대 깊이를 정한다\n\t- 디폴트는 None으로 완벽한 클래스 결정 값이 될 때까지 깊이를 계속 키운다\n- `max_leaf_nodes`\n\t- 말단 노드(leafts)의 최대 갯수를 정한다\n\n### 결정 트리 모델의 시각화\n```python\n#DecisionTreeClassifier 생성하기\ndt_clf= DecisionTreeClassifier(random_state=156)\n\n\n#붗꽃 데이터를 로딩하고, 학습과 테스트 데이터 셋으로 분리하기\niris_data= load_iris()\nX_train, X_test, y_train, y_test= train_test_split(iris_data.data,iris_data.target,test_size=0.2,random_state=11)\n\n#DecisionTreeClassifier 학습\ndt_clf.fit(X_train,y_train)\n\n#graphviz 모듈을 이용하여 시각화하기\n\nfrom sklearn.tree import export_graphviz\n\nexport_graphviz(dt_clf, out_file='tree.dot', class_names= iris_data.target_names,\\\nfeature_names=iris_data.feature_names, impurity=True, filled=True)\n\nimport graphviz\nwith open('tree.dot') as f:\ndot_graph= f.read()\ngraphviz.Source(dot_graph)\n\n______________________________\nresult)\n```\n\n\n",
   "metadata": {
    "cell_id": "30a8d2b6c43a4b56ae1ae43e8ec930c1",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 3426.015625
   }
  },
  {
   "cell_type": "markdown",
   "source": "- 트리 시각화 하기\n    - [트리](https://embed.deepnote.com/b04f8dfd-b00d-42eb-95d2-206667455040/f4cd2ad2-fb90-4aa8-85a9-e83c1ffe6d7c/2e94912ba5fb4e9c8f75d3ab89526ac3?height=797)\n- 결정 트리 알고리즘의 중요도를 추출 및 그래프 시각화 하기\n    - [그래프](https://embed.deepnote.com/b04f8dfd-b00d-42eb-95d2-206667455040/f4cd2ad2-fb90-4aa8-85a9-e83c1ffe6d7c/f65d4b0c3839482cb89ae8f04c4d2420?height=906.765625)\n- 2개의 피처를 나타낸 좌표평면상에 3개의 클래스가 어떻게 분류되었는지 시각화하기\n    - [피쳐와 클래스](https://embed.deepnote.com/b04f8dfd-b00d-42eb-95d2-206667455040/f4cd2ad2-fb90-4aa8-85a9-e83c1ffe6d7c/907a92188dde4b90bd10374bc0b6222a?height=828)\n",
   "metadata": {
    "cell_id": "9d9236a564a745b688a9ee08b2685055",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 228
   }
  },
  {
   "cell_type": "markdown",
   "source": "",
   "metadata": {
    "cell_id": "294ab59eba3647e4915fc48e7730938a",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 46
   }
  },
  {
   "cell_type": "markdown",
   "source": "## 앙상블 학습\n- 앙상블 학습이란?\n    - 여러개의 분류모델을 만들고, \u001f각 모델들의 예측결과들을 결합하여 보다 정확한 예측결과를 도출해내는 기법이다\n    - 이미지,영상,음성등의 비정형 데이터의 분류는 딥러능이 뛰어난 성능을 보이고 있지만, 대부분의 정형 데이터 분류에선 앙상블 학습이 뛰어난 성능을 보이고 있다\n    - 현재 Kaggle에서 XGBoost,LightGBM등을 비롯한 다양한 유형의 앙상블 학습등이 머신러닝의 선도 알고리즘으로 인기를 끌고 있다\n    - 앙상블 학습의 방식으로 보팅 Voting, 배깅 Bagging, 부스팅 Boosting 이 세가지가 대표적으로 존재한다\n\n## 1. 보팅\n\n- 보팅의 유형: 하드 보팅과 소프트 보팅\n    - 하드 보팅 Hard Voting: 다수결의 원칙에 따라 결정. 예측한 결과값중 다수의 분류기가 결정한 예측값을 최종 보팅 결과값으로 선정한다\n    - 소프트 보팅 Soft Voting: 각 분류기\u001d의, 각 레이블에 대한 예측 확률들을 평균내어 가장 높은 값을 최종 보팅 결과값으로 결정하는 방식\n         \n\n\n    ",
   "metadata": {
    "cell_id": "20e6a134f77b454fb496476ae1bdbdc3",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 488
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "fcebc05d99f24536ad247247f9fc2857",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "f40d4a29",
    "execution_start": 1658626402405,
    "execution_millis": 1030,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 261
   },
   "source": "# 예제: 유방암 데이터셋 진단에 VotingClassifier 사용하기\nimport pandas as pd \nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\ncancer= load_breast_cancer()\n",
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "2b96daf4fd1a4ddb949d41d13df84ea5",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "aad20ea5",
    "execution_start": 1658626403445,
    "execution_millis": 66,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 350.5
   },
   "source": "# 개별 학습 모델은 로지스틱 회귀와 KNN이다\nlr_clf= LogisticRegression(solver='liblinear')\nknn_clf= KNeighborsClassifier(n_neighbors=8)\n\n#개별 모델을 소프트 보팅 기반의 앙상블 모델로 구현한 분류기\nvo_clf= VotingClassifier(estimators=[('LR',lr_clf),('KNN',knn_clf)],voting='soft')\n\nX_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,\\\n                                test_size=0.2, random_state=156)\n\n# VotingClassifier 학습/예측/평가\nvo_clf.fit(X_train,y_train)\npred= vo_clf.predict(X_test)\nprint(f'Voting 분류기 정확도는 {accuracy_score(y_test,pred):.3f}')",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": "Voting 분류기 정확도는 0.956\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "6b8ba862d2414e71818c681582a9a9dd",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "979932c6",
    "execution_start": 1658626403518,
    "execution_millis": 136,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 267
   },
   "source": "# 개별 모델의 학습/예측/평가\nclassifiers= [lr_clf,knn_clf]\nfor classifier in classifiers:\n    classifier.fit(X_train,y_train)\n    pred= classifier.predict(X_test)\n    class_name= classifier.__class__.__name__\n    print(f'{class_name}의 정확도는 {accuracy_score(y_test,pred):.3f}')\n",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": "LogisticRegression의 정확도는 0.947\nKNeighborsClassifier의 정확도는 0.939\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 2. 배깅\n- 배깅이란?\n    - 배깅은 같은 알고리즘이지만, 다른 데이터셋을 갖는 classifier이 개별적으로 학습을 수행한 뒤 보팅으로 최종 결정하는 알고리즘이다. 이때 개별 트리의 데이터셋은 전체 데이터 셋의 일부가 중첩되게 샘플링된다(부트스트래핑bootstrapping 분할 방식)\n    - 배깅 알고리즘에 대표적으로 랜덤 포레스트가 있다\n    - 일반적으로 가장 앙상블 알고리즘 중 가장 빠른 수행속도를 가지고 있으며 다양한 영역에서 높은 예측 성능을 갖고 있다\n- 랜덤 포레스트\n    - [랜덤 포레스트 예시](https://embed.deepnote.com/b04f8dfd-b00d-42eb-95d2-206667455040/f4cd2ad2-fb90-4aa8-85a9-e83c1ffe6d7c/16d181282a64436e9cceb5a432166d02?height=920)\n    - [랜덤 포레스트-하이퍼파라미터 튜닝](https://embed.deepnote.com/b04f8dfd-b00d-42eb-95d2-206667455040/f4cd2ad2-fb90-4aa8-85a9-e83c1ffe6d7c/b2bf40adec034559b3a2fafc2e1123ff?height=493)\n    - [Feature_importances sorting 시각화](https://embed.deepnote.com/b04f8dfd-b00d-42eb-95d2-206667455040/f4cd2ad2-fb90-4aa8-85a9-e83c1ffe6d7c/29ed2d766de4462ca9be6692b962a4c5?height=539.890625)\n    - 랜덤 포레스트의 하이퍼파라미터\n        - `n_estimators`: 결정 트리의 개수를 지정한다. default는 10개로 정해져있다. 많이 설정할 수록 좋은 성능을 기대할 수도 있지만 계속 증가시킨다고 해서 성능이 무조건 향상되지는 않을뿐더러, 학습시간이 증가하게 된다\n        - `max_features`: 고려할 피처\u001d들을 정한다. 단 디폴트는 None이 아니라 $sqrt$ (전체 피처갯수)이다       \n         \n\n    \n",
   "metadata": {
    "cell_id": "48c67ce8eea64aa093847cc3c48ad6c4",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 453
   }
  },
  {
   "cell_type": "markdown",
   "source": "## GBM(Gradient Boosting Machine)\n- GBM이란?\n    - 여러개의 약한 학습기weak learner를 순차적으로 학습-예측하면서 잘못 예측한 데이터에 가중치를 부여하면서 오류를 개선 및 학습해 나가는 방식이다\n    - GBM에 대표적으로 AdaBoosting(Adaptive Boosting)과 Gradient Boosting이 존재한다\n- AdaBoost\n    - ![Picture title](/레퍼런스/image-20220723-085350.png)\n    - 첫번째 학습에서 하단 원 두개가 잘못 분류되어 두번째 학습에서 가중치가 가해진다.\n    - 두번째 학습에서 오른쪽 원 3개가 잘못 분류되어 세번째 학습에서 가중치가 가해진다\n    - 마지막으로 모든 분류기준을 결합하여 예측을 시행한다\n- GBMB\n    - 에이다부스트와 유사하나 가중치 업데이트로 경사하강법Gradient Descent를 활용한다\n    - GBM의 하이퍼 파라미터\n        - `loss`: 경사 하강법에서 사용할 비용함수를 지정한다. default로 `deviance`를 사용한다\n        - `learning_rate`: GBM이 학습을 진행할 때 마다 적용하는 학습률을 지정한다. 0~1사이를 적용시킬 수 있으며, 기본값은 0.1이 된다. 너무 작은 값을 적용시키면 수행시간이 길어지게 되고, 너무 큰 값을 적용하게 되면 최소 오류값을 찾지 못해 예측 성능이 떨어질 가능성이 높아진다. \n        - `n_estimators`: `weak_learner`의 개수를 지정한다. `weak_learner`가 순차적으로 오류를 보정하므로 개수가 많을수록 예측 성능이 일정 수준까지 좋아질 수 있다. 하지만 개수가 많아질수록 수행시간이 길어진다. default로 100개이다\n        - `subsample`: `weak_learner`가 사용하는 데이터의 샘플링 비율이다. 기본값은 1이며, 이는 전체 학습 데이터를 기반으로 학습한다는 의미이다.과적합이 염려되는 경우 `subsmaple`를 1보다 작은 값으로 설정한다\n\n## XGBoost\n- XGBoost란?\n    - 트리 기반 앙상블 학습에서 각광받고 있는 알고리즘중 하나로, Kaggle 상위권 데이터 과학자들이 많이 활용하게 되면서 유명해지게 되었다\n    - 표준 GBM과 달리 과적합 규제기능이 존재한다\n    - 지정된 반복횟수가 아니라 교차 검증을 통해 평가 데이터 셋의 평가값이 최적화 되면 반복을 중간에 멈출수 있는 조기 중단 기능이 존재한다\n    - 별도로 존재하는 프레임워크이나 사이킷런과 연동하기 위해 전용 래퍼 클래스를 개발하였고, 사이킷런에서 활용가능한 메소드 XGBClassifier는 기존 사이킷런의 하이퍼파라미터명과의 일관성을 위해 다음과 같이 변경되었다\n        - `eta` -> `learning_rate`\n        - `sub_sample` -> `subsample`\n        - `lambda` -> `reg_lambda`\n        - `alpha` -> `reg_alpha`\n- XGBoost의 하이퍼파라미터\n    - 일반 파라미터\n        - `booster`: booster를 결정한다 gbtree(tree based model) 또는 gblinear(linear model)로 결정한다\n        - `silent` : 출력 메세지를 나타내고 싶으면 1 , 아니면 0으로 한다. 디폴트는 0이다\n        - `nthrread`: CPU의 실행 스레드 개수를 조정한다. 디폴트는 CPU의 전체 스레드를 사용한다\n    - 부스터 파라미터\n        - `eta[default=0.3,alias:learning_rate]`: GBM의 학습률과 같은 파라미터. 0과 1사이 값을 지정하며, 부스팅 스텝을 반복적으로 수행할 때에 업데이트 되는 학습률 값이다.\n        - `num_boost_rounds`: GBM의 n_estimators와 같은 파라미터이다\n        - `min_child_weigh[default=1]`: 트리에서 추가적으로 가지를 나눌지를 결정하기 위한 데이터 weigh의 총합 \n        - `gamma['default=0, alias:min_split_loss']`:트리의 리프 노드를 추가적으로 나눌지를 결정하는 최소 손실 감소 값. 해당 값보다 loss가 클 경우에 리프 노드를 분리한다\n        - `max_depth[default=6]`: 트리 기반 알고리즘의 `max_depth`와 같다. 0을 지정하면 깊이 제한이 없는 것으로 정하는 것이다. 이 값이 클수록 과적합 가능성이 높아지며 일반적으로 3~10 사이 값을 사용한다\n        - `sub_sample[default=1']`:GBM의 `subsample`과 같다. 트리가 커져 과적합되는 것을 방지하기 위해 데이터 샘플링하는 비율을 지정한다. 0과 1사이 값을 지정할 수 있다\n        - `colsample_bytree[default=1]`:GBM의 max_features와 유사하다. 트리 샘플에 필요한 피처를 임의로 샘플링 하는데 사용한다\n        - `lambda[default=1 alias:reg_lambda]`: L2 regularization의 적용값을 지정한다. 값이 클수록 과적합 감소 효과가 있다\n        - `alpha[default=0, alias:reg_alpha]`: L1 regularization 적용값이다. 값이 클수록 과적합 감소 효과가 존재한다\n        - `scale_pos_weight[default=1]`: 특정 값으로 치우친 비대칭한 클래스로 구성된 데이터 셋의 균형을 유지하기 위한 하이퍼파라미터이다\n\n    - 학습 태스트 파라미터\n        - `objective`: 최솟값을 가져야 할 손실함수를 정의한다\n        - `binary:logistic`:이진분류일 때 적용한다\n        - `multi:softmax` : 다중분류일 떄 적용한다\n        - `multi:softprob`: 개별 레이블 클래스에 해당되는 예측 확률을 반환한다\n        - `eval_metric`:검증에 사용되는 함수를 정의한다. 기본값은 회귀읜 경우 `rmse`, 분류인 경우 `error`이다.  \n        - `rmse`: Root Mean Square Error\n        - `mae` : Mean Absolute Error\n        - `logloss`:Negative log-likelihood\n        - `error`:Binary classification error rate(0.5 threshold)\n        - `merror`: Multiclass classification error rate\n        - `mlogloss`: Multiclass logloss\n        - `auc`: Area under Curve\n            \n## LightGBM\n- LightGMB이란 무엇인가?\n    - XGBoost보다 학습에 걸리는 시간이 훨씬 적고, 메모리 사용량도 상대적으로 적다\n    - 그럼에도 예측성능은 XGboost와 비슷하며, 기능상의 다양성은 LightBGM이 더 많이 존재한다\n    - 다만 10,000건 이하의 적은 데이터셋의 경우 과적합 문제가 발생하기 쉬울 수 있다\n- 어떻게 이런 기술이 가능했는가?\n    - 기존 GBM 계열의 트리 분할 방식은 리프 중심 분할(Leaf Wise) 방식이라 하여, 최대한 트리의 균형을 맞추며 분할하기 때문에 트리의 깊이를 최소화되고, 오버피팅을 예방한다 -> 그러나 이 방식은 균형을 맞추는데 시간이 필요하단 단점이 존재한다\n    - 그러나 LightGBM은 최대 손실 값(max delta loss)을 가진 리프 노드를 지속적으로 분할하면서 비대칭적인 트리가 생성된다\n    - LightGBM의 하이퍼파라미터\n        - `num_iteration[default=100]`:반복 수행하려는 트리의 개수를 지정한다. 크게 지정할수록 예측 성능이 높아질 수 있으나, 너무 크면 과적합으로 성능이 저하될 수 있다\n        - `learning_rate[default=0.1]`: 0 ~ 1 사이 값을 지정한다. 부스텡 스텝을 반복적으로 수행할 때 업데이트 되는 학습률 값이다. \n        - `max_depth[default=-1]`: 트리 기반 알고리즘의 `max_depth`와 동일하다.\n        - `min_data_in_leaf[default=20]`: 최종 결정 클래스인 리프 노드가 되기 위해 최소한으로 필요한 데이터의 수. 과적합을 제어하기 위한 파라미터이다\n        - `num_leaves[default=31]`: 하나의 트리가 가질 수 있는 최대 리프 개수\n        - `boosting`: 부스팅 트리를 생성할 수 있는 알고리즘\n            -`gbdt`:일반적인 그레디언트 부스팅 결정 트리\n            -`rf`: 랜덤 포레스트\n        - `bagging_fraction[default=1.0]`:트리가 과적합되는 것을 제어하기 위해 데이터를 샘링하는 비율.\n        - `feature_fraction['default=1.0']`: 개별 트리를 학습할 때마다 무작위로 선택하는 피처의 비율\n        - `lambda_l2[default=0.0]`: L2_regularization 제어를 위한 값, 과적합 제어를 위하여 사용한다\n        - `lambda_l1[default=0.0]`: L1_regularization 제어를 위한 값. 과적합 제어를 위하여 사용한다\n    ",
   "metadata": {
    "cell_id": "36c05d90ba4b421da914be613ab5efb1",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 3247
   }
  },
  {
   "cell_type": "markdown",
   "source": "",
   "metadata": {
    "cell_id": "6fc9ccf5f8164a91873af73a074daff3",
    "tags": [],
    "owner_user_id": "6620e961-4734-4451-b6f0-024902f1a0c6",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 46
   }
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=5a548db0-9bec-4ed6-b090-9c062f32efdb' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {},
  "deepnote_notebook_id": "487fca90-cc0e-4caf-b545-ccd0616de4f8",
  "deepnote_execution_queue": []
 }
}